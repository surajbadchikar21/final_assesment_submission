{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c54a1bdf-e4d5-4f23-ab03-80e641094aef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.ipldataadlsg2.dfs.core.windows.net\",\n",
    "    \"TPpjUJkEsQms7TPvRF5yFJb5fdzcy6Sdn8qLaN2S8KLQ7Jufa9D2lupgDXQQfc2VoP9NpQjexWaH+AStTcTc0Q==\"\n",
    ")\n",
    "\n",
    "\n",
    "def list_files_in_blob(container_name, storage_account, folder_path=\"\"):\n",
    "    '''This function helps user to list all files in a given  path and return a list of file paths'''\n",
    "    blob_path = f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/{folder_path}\"\n",
    "    try:\n",
    "        files = dbutils.fs.ls(blob_path)\n",
    "        return [file.path for file in files]\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []\n",
    "\n",
    "def read_file(blob_path):\n",
    "    '''This function helps uer to read a file from a given path and return a Spark DataFrame and the source file name '''\n",
    "    try: \n",
    "        df= spark.read.format(\"csv\").options(header=\"true\", inferSchema=\"true\").load(blob_path)\n",
    "        source_file=blob_path.split(\"/\")[-1]\n",
    "        return df,source_file\n",
    "    except Exception as e:\n",
    "       print(f\"Error: {e}\")\n",
    "       return None,None\n",
    "   \n",
    "def write_file(df, blob_base_path, source_file):\n",
    "    ''' This function helps user to write a Spark DataFrame to a given path and return the source file name while adding audit columns(ingestion_date, source_file)\n",
    "    '''\n",
    "    try:\n",
    "        base_name = source_file.split(\".\")[0]\n",
    "        full_path = f\"{blob_base_path}/{base_name}_bronze\"\n",
    "\n",
    "        ingestion_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        df = df.withColumn(\"ingestion_date\", lit(ingestion_date)) \\\n",
    "            .withColumn(\"source_file\", lit(source_file))\n",
    "\n",
    "        df.write.format(\"parquet\").option(\"header\", \"true\").save(full_path)\n",
    "        print(f\"Data written to: {full_path}\")\n",
    "    except Exception as e:\n",
    "       print(f\"Error: {e}\")\n",
    "       return None,None\n",
    "\n",
    "def delete_file_from_blob(blob_path):\n",
    "    '''Deletes a file or folder from Azure Blob Storage using full abfss path.'''\n",
    "    try:\n",
    "        dbutils.fs.rm(blob_path, True)\n",
    "        print(f\"Successfully deleted: {blob_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting file: {e}\")\n",
    "\n",
    "\n",
    "# cleaning and transformations \n",
    "from pyspark.sql.functions import col, sum as s, when\n",
    "def null_counts(df):\n",
    "    ''' Counts the number of null values in each column of a Spark DataFrame and returns a new DataFrame'''\n",
    "    return df.select([\n",
    "        s(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    "\n",
    "\n",
    "def write_file_to_silver(df, source_file):\n",
    "    \"\"\"\n",
    "    Writes a Spark DataFrame to the 'ipl-silver' container on 'ipldataadlsg2' storage account.\n",
    "    Ensures it creates a folder, not a file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        storage_account = \"ipldataadlsg2\"\n",
    "        container = \"ipl-silver\"\n",
    "        base_name = source_file.split(\".\")[0]\n",
    "        full_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{base_name}-silver/\"  \n",
    "        df.write.format(\"parquet\").option(\"header\", \"true\").mode(\"overwrite\").save(full_path)\n",
    "    except Exception as e:\n",
    "       print(f\"Error: {e}\")\n",
    "       return None,None\n",
    "\n",
    "def write_df_to_sql(df, table_name):\n",
    "    ''' This function helps user to write a Spark DataFrame to a given table name in a SQL database'''\n",
    "    try:\n",
    "        df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", jdbc_url) \\\n",
    "            .option(\"dbtable\", f\"dbo.{table_name}\") \\\n",
    "            .option(\"user\", user) \\\n",
    "            .option(\"password\", password) \\\n",
    "            .option(\"driver\", driver) \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "    except Exception as e:\n",
    "       print(f\"Error: {e}\")\n",
    "       return None,None"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "common_functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
